# ğŸ“š Apuntes: Ecosistema Hadoop (YARN & MapReduce)

Este documento resume la evoluciÃ³n, arquitectura y funcionamiento del procesamiento de datos en Hadoop 2.0+.

---

## 1. EvoluciÃ³n: De MapReduce 1 a YARN ğŸš€

En la versiÃ³n antigua de Hadoop (MR1), un solo componente (**JobTracker**) lo hacÃ­a todo, lo que generaba cuellos de botella al superar los **5,000 nodos**.  
Con la llegada de **YARN (Hadoop 2.0)**, se separaron las responsabilidades.

### ğŸ”„ Tabla comparativa: El cambio de paradigma

| Concepto | MapReduce 1 (Obsoleto) | YARN (Moderno) |
|---|---|---|
| GestiÃ³n | **JobTracker**: Gestionaba recursos y monitoreaba tareas simultÃ¡neamente. | **ResourceManager**: Solo gestiona recursos globales (CPU/RAM). |
| EjecuciÃ³n | **TaskTracker**: Ejecutaba tareas en nodos esclavos. | **NodeManager**: Gestiona los recursos de su nodo especÃ­fico. |
| Recursos | **Slot**: Huecos fijos y rÃ­gidos para tareas. | **Container**: Paquete dinÃ¡mico y abstracto de RAM y CPU. |

ğŸ’¡ **La clave:** YARN actÃºa como el â€œSistema Operativoâ€ del clÃºster, permitiendo que no solo corra MapReduce, sino tambiÃ©n **Spark** o **Streaming**.

---

## 2. Arquitectura de YARN: Los 3 niveles de mando ğŸ›ï¸
flowchart TB
  RM[ğŸ‘‘ ResourceManager\n(gestiona recursos globales)] --> NM1[ğŸ‘· NodeManager\n(nodo trabajador)]
  RM --> NM2[ğŸ‘· NodeManager\n(nodo trabajador)]
  RM --> NM3[ğŸ‘· NodeManager\n(nodo trabajador)]

  AM[ğŸ¼ ApplicationMaster\n(1 por aplicaciÃ³n)] --> RM
  AM --> NM1
  AM --> NM2
  AM --> NM3

  NM1 --> C1[ğŸ“¦ Containers\n(tareas)]
  NM2 --> C2[ğŸ“¦ Containers\n(tareas)]
  NM3 --> C3[ğŸ“¦ Containers\n(tareas)]

**YARN (Yet Another Resource Negotiator)** gestiona el hardware del clÃºster mediante tres componentes principales:

### ğŸ‘‘ ResourceManager (El DueÃ±o)
- Ubicado en el nodo maestro.
- Tiene autoridad mÃ¡xima sobre los recursos (RAM/CPU) de todo el clÃºster.
- Contiene un **Scheduler** (planificador) que decide quiÃ©n recibe recursos.

### ğŸ‘· NodeManager (El Capataz)
- Hay uno en cada nodo esclavo (trabajador).
- Vigila su propia mÃ¡quina y reporta el estado al ResourceManager.
- Crea los **Contenedores** donde se ejecutan las tareas finales.

### ğŸ¼ ApplicationMaster (El Director de Orquesta)
- Se crea uno por cada aplicaciÃ³n o trabajo lanzado.
- Negocia recursos con el ResourceManager y da Ã³rdenes a los NodeManagers.
- Desaparece en cuanto el trabajo termina.

---

## 3. ConfiguraciÃ³n y administraciÃ³n âš™ï¸

Para que el sistema funcione, es necesario configurar y arrancar los servicios correctamente.

### ğŸ“„ Archivos de configuraciÃ³n (`/etc/hadoop`)
- `mapred-site.xml`: Se debe especificar que el framework es **yarn**.
- `yarn-site.xml`: Se define el hostname del ResourceManager y se activa el servicio `mapreduce_shuffle`.

### ğŸš€ Orden de arranque
1. **HDFS:** `start-dfs.sh` (Levanta NameNode y DataNodes).
2. **YARN:** `start-yarn.sh` (Levanta ResourceManager y NodeManagers).
3. **History Server (Opcional):** `mr-jobhistory-daemon.sh start historyserver`  
   - Utilidad: Permite ver los logs de trabajos ya terminados.

### ğŸŒ Interfaz web
Puedes monitorear todo en tiempo real en:  
`http://localhost:8088`

---

## 4. MapReduce: El flujo de trabajo ğŸ› ï¸

flowchart LR
  IN[ğŸ“¥ Input (HDFS)] --> MAP[ğŸ” Mapper\n(clave, valor)]
  MAP --> SHUF[ğŸ”€ Shuffle & Sort\n(agrupa por clave)]
  SHUF --> RED[ğŸ“Š Reducer\n(operaciÃ³n final)]
  RED --> OUT[ğŸ“¤ Output (HDFS)]
MapReduce es el paradigma para procesar datos en paralelo. Se divide en tres fases estrictas:

### Fase 1: Mapper (El Clasificador) ğŸ”
- **Entrada:** Datos en bruto (lÃ­neas de texto).
- **AcciÃ³n:** Filtra y emite pares **Clave-Valor** (Ej: `pepe, 1`).
- **Nota:** No realiza cÃ¡lculos globales, solo clasifica.

### Fase 2: Shuffle & Sort (El Organizador) ğŸ”€
- **AcciÃ³n:** Proceso automÃ¡tico que recoge las salidas de los mappers, las ordena alfabÃ©ticamente y las agrupa por clave.
- **Resultado:** El Reducer recibe algo como: `pepe, [1, 1, 1]`.

### Fase 3: Reducer (El Contador) ğŸ“Š
- **AcciÃ³n:** Itera sobre la lista de valores y realiza la operaciÃ³n final (suma, media, etc.).
- **Salida:** El resultado final se guarda en **HDFS**.

---

## 5. MapReduce con Python (Hadoop Streaming) ğŸ

Aunque nativamente se usa Java, **Hadoop Streaming** permite usar Python mediante la entrada y salida estÃ¡ndar (`stdin/stdout`).  
Se usa principalmente por su potencia en IA y librerÃ­as de datos.

### ğŸ“ Comando de ejecuciÃ³n (Bash)

```bash
hadoop jar /ruta/a/hadoop-streaming.jar \
  -files mapper.py,reducer.py \        # EnvÃ­a los scripts a los nodos
  -mapper mapper.py \                  # Script para la fase Map
  -reducer reducer.py \                # Script para la fase Reduce
  -input /entrada/hdfs \               # Origen de datos
  -output /salida/hdfs                 # Destino de resultados

  ---

## 6. Estrategias de OrdenaciÃ³n (Sorting) ğŸ“‰

MapReduce ordena por **clave** automÃ¡ticamente. Â¿Pero quÃ© pasa si queremos ordenar por **valor** (ej: ranking de palabras mÃ¡s repetidas)?

### A. Ordenar en Linux (Archivos pequeÃ±os) ğŸ§
[cite_start]Si el resultado final es pequeÃ±o (<128MB), es mÃ¡s rÃ¡pido bajarlo a local y ordenar con comandos de sistema [cite: 416-418].

```bash
hdfs dfs -cat /salida/part-* | sort -k2,2n > top_usuarios.txt

```Notas:

-k2,2n: ordena por la segunda columna (2), tratÃ¡ndola como nÃºmero (n).

-r: aÃ±Ã¡delo si quieres orden inverso (descendente), por ejemplo:

hdfs dfs -cat /salida/part-* | sort -k2,2nr > top_usuarios.txt
B. Ordenar en Hadoop (archivos gigantes) ğŸ˜

Para Big Data real, se debe configurar el trabajo para que ordene globalmente usando comparadores especÃ­ficos.

Requiere usar la clase KeyFieldBasedComparator.

Se configura con opciones -D en el comando de ejecuciÃ³n.

Ejemplo (plantilla orientativa con -D):
```bash
hadoop jar /ruta/a/hadoop-streaming.jar \
  -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
  -D mapreduce.partition.keycomparator.options="-k2,2nr" \
  -files mapper.py,reducer.py \
  -mapper mapper.py \
  -reducer reducer.py \
  -input /entrada/hdfs \
  -output /salida/hdfs
  ```

